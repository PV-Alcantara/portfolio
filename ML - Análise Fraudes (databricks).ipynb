{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c47a0cf-6e8e-4549-9d32-e367564b15ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instalação de App"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5665d713-5f3f-496e-8aeb-3f542d6508e7/lib/python3.12/site-packages (24.0)\nCollecting pip\n  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/1.8 MB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m36.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m21.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.0\n    Uninstalling pip-24.0:\n      Successfully uninstalled pip-24.0\nSuccessfully installed pip-25.1.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[33mDEPRECATION: Using the pkg_resources metadata backend is deprecated. pip 26.3 will enforce this behaviour change. A possible replacement is to use the default importlib.metadata backend, by unsetting the _PIP_USE_IMPORTLIB_METADATA environment variable. Discussion can be found at https://github.com/pypa/pip/issues/13317\u001B[0m\u001B[33m\n\u001B[0mRequirement already satisfied: python-slugify in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (8.0.4)\nRequirement already satisfied: loguru in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (0.7.3)\nRequirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (3.1.5)\nRequirement already satisfied: scikit-learn in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (1.3.2)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (1.4.2)\nRequirement already satisfied: shap in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (0.47.2)\nCollecting imbalanced-learn\n  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: text-unidecode>=1.3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from python-slugify) (1.3)\nRequirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.5.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from shap) (1.5.3)\nRequirement already satisfied: tqdm>=4.27.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /databricks/python3/lib/python3.12/site-packages (from shap) (24.1)\nRequirement already satisfied: slicer==0.0.8 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from shap) (0.0.8)\nRequirement already satisfied: numba>=0.54 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from shap) (0.61.2)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.12/site-packages (from shap) (2.2.1)\nRequirement already satisfied: typing-extensions in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from shap) (4.13.2)\nCollecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from numba>=0.54->shap) (0.44.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->shap) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->shap) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\nDownloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\nDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\nInstalling collected packages: sklearn-compat, imbalanced-learn\n\u001B[?25l\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [imbalanced-learn]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/2\u001B[0m [imbalanced-learn]\n\u001B[?25h\n\u001B[1A\u001B[2KSuccessfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-slugify\n",
    "#!pip install loguru\n",
    "#!pip install openpyxl\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install python-slugify loguru openpyxl scikit-learn joblib shap imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed53ecd-3d4c-4f58-9419-29402ef8c6bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bibliotecas"
    }
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "from slugify import slugify\n",
    "from functools import partial\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql.functions import split, trim, col, substring\n",
    "from pyspark.sql.types import StringType, DateType, FloatType, IntegerType\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad9ed44-c670-41ad-a8ad-6ece7215375a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sharepoint -  Conecta com Sharepoint"
    }
   },
   "outputs": [],
   "source": [
    "## Permite a conexão com Sharepoint\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "delta = SparkSession.builder.appName(\"\").getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "secret_dict = {\n",
    "'Databricks':'key-trdg-bricks-token',\n",
    "'Sharepoint':'key-trdg-client-secret',\n",
    "'FTP':'key-trdg-ftp',\n",
    "'SAS':'key-trdg-sas',\n",
    "}\n",
    "\n",
    "def get_secret(which: str):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"keyvault\", key=secret_dict[which])\n",
    "    except:\n",
    "        print('The secrets available are: Databricks[Token], Sharepoint[Token], FTP[Password] and SAS[User and Password].')\n",
    "\n",
    "\n",
    "URL_TOKEN = 'https://login.microsoftonline.com/24090322-b104-494d-a1d3-662da14cddd4/oauth2/v2.0/token'\n",
    "URL_MSG = 'https://graph.microsoft.com/v1.0/sites/'\n",
    "\n",
    "BLOB_PATH = 'abfss://sandbox@adltrdgwestus.dfs.core.windows.net/'\n",
    "\n",
    "#spark.conf.set(\n",
    "#    \"fs.azure.account.key.adltrdgwestus.dfs.core.windows.net\",\n",
    "#    dbutils.secrets.get(scope=\"scope-keyvault-prd\", key=\"secret-databricks-trdg-prd\"))\n",
    "\n",
    "\n",
    "class Sharepoint(ABC):\n",
    "    \"\"\"Class that implements the logic to extract and load\n",
    "    datasets from the Sharepoint to the Databricks.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_file: str, name_site: str = 'DadosBI', host: str = 'achelaboratorios.sharepoint.com'):\n",
    "        self.path_file = path_file \n",
    "        self.name_site = name_site \n",
    "        self.host = host\n",
    "\n",
    "        self.headers = self.get_bearer_token()\n",
    "        self.data = self.download_file()\n",
    "\n",
    "    \n",
    "    def get_bearer_token(self) -> dict:\n",
    "        body = {'grant_type': 'client_credentials', \n",
    "                'client_id': '01c78346-6928-48c4-8cd6-d0ef71ec7021', \n",
    "                'client_secret': get_secret(which='Sharepoint'),\n",
    "                'scope': 'https://graph.microsoft.com/.default'\n",
    "                }\n",
    "        \n",
    "        headers = CaseInsensitiveDict()\n",
    "        headers[\"Accept\"] = \"application/json\"\n",
    "        headers[\"Authorization\"] = f\"Bearer {json.loads(requests.post(URL_TOKEN, data=body).text)['access_token']}\"\n",
    "\n",
    "        return headers\n",
    "\n",
    "\n",
    "    def get_ids(self, is_file=True) -> str:\n",
    "        id_site = (requests\n",
    "                   .get(f'{URL_MSG}/{self.host}:/sites/'+self.name_site+'?$select=id', \n",
    "                        headers=self.headers)\n",
    "                   .json()['id']\n",
    "                   .split(',')\n",
    "                   [1]\n",
    "                   )\n",
    "        id_drives = (requests\n",
    "                    .get(URL_MSG+id_site+f\"/drive\", \n",
    "                                 headers=self.headers)\n",
    "                    .json()\n",
    "                    ['id']\n",
    "                    )\n",
    "        if is_file:\n",
    "            url_content = URL_MSG+id_site+'/drives/'+id_drives+f'/root:/{self.path_file}:/content'\n",
    "        else: \n",
    "            url_content = URL_MSG+id_site+'/drives/'+id_drives+f'/root:/{self.path_file}:/children'\n",
    "        return url_content\n",
    "    \n",
    "\n",
    "    def download_file(self) -> dict:\n",
    "        if '.' in self.path_file:\n",
    "            url_content = self.get_ids()\n",
    "        else:\n",
    "            url_content = self.get_ids(is_file=False)\n",
    "        return (requests.get(url_content, headers=self.headers)).content\n",
    "    \n",
    "    def ls(self) -> list:\n",
    "        data_dict = json.loads(self.data)\n",
    "        list_files = []\n",
    "        for files in data_dict['value']:\n",
    "            list_files.append(files['name'])\n",
    "        return list_files\n",
    "    \n",
    "    def read_file(self, enconding:str = 'utf-8', **params) -> pd.DataFrame:\n",
    "        type_file = self.path_file.split('.')[1]\n",
    "        if type_file in ['txt', 'csv']:\n",
    "            self.read_data = pd.read_csv(io.StringIO(self.data.decode(enconding)), **params)\n",
    "            return self\n",
    "        elif type_file in ['xls', 'xlsx']:\n",
    "            self.read_data = pd.read_excel(self.data, **params)\n",
    "            return self\n",
    "        else:\n",
    "            print('Este tipo de arquivo não está implementado!')\n",
    "    \n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        return self.read_data\n",
    "    \n",
    "\n",
    "    def export_blob_storage_csv(self, path:str, delimiter:str = ';', encoding:str = \"UTF-8\") -> str:\n",
    "        ps.from_pandas(self.read_data).to_spark() \\\n",
    "        .coalesce(1) \\\n",
    "            .write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"delimiter\", delimiter) \\\n",
    "            .option(\"encoding\",encoding) \\\n",
    "            .csv(BLOB_PATH + path)\n",
    "\n",
    "\n",
    "    def slugify_columns(self, columns):\n",
    "        slug = partial(slugify, separator=\"_\")\n",
    "        return [slug(column.replace(\"%\", \"percent\").replace(\"+\", \"_\")) for column in columns]\n",
    "\n",
    "\n",
    "    def export_blob_storage_parquet(self, path:str, compression:str = 'snappy') -> str:\n",
    "        df = self.read_data\n",
    "        df.columns = self.slugify_columns(df.columns)\n",
    "        ps.from_pandas(df).to_spark() \\\n",
    "        .coalesce(1) \\\n",
    "            .write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"compression\", compression) \\\n",
    "            .parquet(BLOB_PATH + path)\n",
    "    \n",
    "    \n",
    "    def export_datalake(self, path) -> str:\n",
    "        ps.from_pandas(self.read_data).to_spark() \\\n",
    "         .write \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .option(\"overwriteSchema\", \"true\") \\\n",
    "         .saveAsTable(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc11bb9-fa65-47fd-82e5-79ae3a855d68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definição de funções"
    }
   },
   "outputs": [],
   "source": [
    "# Função para pré-processar os dados\n",
    "def preprocess_data(df):\n",
    "    logger.info(\"Iniciando pré-processamento...\")\n",
    "    df = df.dropna()\n",
    "    features = ['SEGMENTO', 'PRIMEIRA_COMPRA', 'PRIMEIRA_COMPRA_CRM_LISTA_DE_BLOQUEIO_perc',\n",
    "                'PRIMEIRA_COMPRA_CPF_INATIVO_perc', 'QTD_MARCA_AVG', 'QTD_MARCA_qt50']\n",
    "    X = df[features].copy()\n",
    "    Y = df['Desvio'].copy()\n",
    "\n",
    "    # Transforma a coluna target em numérico\n",
    "    Y = Y.map({'N': 0, 'S': 1}).astype(int)\n",
    "\n",
    "    X['SEGMENTO'] = LabelEncoder().fit_transform(X['SEGMENTO'])\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    logger.success(\"Pré-processamento finalizado.\")\n",
    "    return X_scaled, Y.values, scaler\n",
    "\n",
    "# Função para treinar o modelo\n",
    "def train_model(X, Y):\n",
    "    logger.info(\"Treinando modelo...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, Y_resampled = smote.fit_resample(X,Y)\n",
    "    rf.fit(X_resampled, Y_resampled)\n",
    "    logger.success(\"Modelo treinado.\")\n",
    "    return rf\n",
    "\n",
    "# Função para validação cruzada\n",
    "def validate_model(model, X, Y):\n",
    "    logger.info(\"Validando modelo com cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(model, X, Y, cv=skf, scoring='accuracy')\n",
    "    logger.success(f\"Validação finalizada. Acurácia média: {scores.mean():.4f}\")\n",
    "    return scores\n",
    "\n",
    "# Função para avaliação no teste\n",
    "def evaluate_model(model, X_test, Y_teste):\n",
    "    logger.info(\"Avaliando modelo...\")\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(classification_report(Y_teste, Y_pred))\n",
    "    print(f\"Accuracy: {accuracy_score(Y_teste, Y_pred):.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(Y_teste, Y_pred):.4f}\")\n",
    "    cm = confusion_matrix(Y_teste, Y_pred)\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "\n",
    "# Função para salvar o modelo e o scaler\n",
    "def save_model(model, scaler, model_path='modelo_rf.pkl', scaler_path='scaler.pkl'):\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    logger.success(\"Modelo e scaler salvos.\")\n",
    "\n",
    "# Função para predição em novos dados\n",
    "def predict_new_data(df_new, model, scaler):\n",
    "    logger.info(\"Realizando predição em novos dados...\")\n",
    "    features = ['SEGMENTO', 'PRIMEIRA_COMPRA', 'PRIMEIRA_COMPRA_CRM_LISTA_DE_BLOQUEIO_perc',\n",
    "                'PRIMEIRA_COMPRA_CPF_INATIVO_perc', 'QTD_MARCA_AVG', 'QTD_MARCA_qt50']\n",
    "    df = df_new[features].copy()\n",
    "    df['SEGMENTO'] = LabelEncoder().fit_transform(df['SEGMENTO'])\n",
    "    X_scaled = scaler.transform(df)\n",
    "    preds = model.predict(X_scaled)\n",
    "    df_new['Desvio_predito'] = preds\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c034ebb-dcf7-4d41-96be-45fa415aebcd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execução"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-05-26 11:01:48.501\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mpreprocess_data\u001B[0m:\u001B[36m3\u001B[0m - \u001B[1mIniciando pré-processamento...\u001B[0m\n\u001B[32m2025-05-26 11:01:48.511\u001B[0m | \u001B[32m\u001B[1mSUCCESS \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mpreprocess_data\u001B[0m:\u001B[36m16\u001B[0m - \u001B[32m\u001B[1mPré-processamento finalizado.\u001B[0m\n\u001B[32m2025-05-26 11:01:48.514\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mtrain_model\u001B[0m:\u001B[36m21\u001B[0m - \u001B[1mTreinando modelo...\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf4ef3f8c6e484ba2eaec224b0ea43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d073885d941a411bba59c060911ebd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8687730f05047e2b412401d03de8252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-05-26 11:02:16.729\u001B[0m | \u001B[32m\u001B[1mSUCCESS \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mtrain_model\u001B[0m:\u001B[36m26\u001B[0m - \u001B[32m\u001B[1mModelo treinado.\u001B[0m\n\u001B[32m2025-05-26 11:02:16.731\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mvalidate_model\u001B[0m:\u001B[36m31\u001B[0m - \u001B[1mValidando modelo com cross-validation...\u001B[0m\n\u001B[32m2025-05-26 11:02:19.042\u001B[0m | \u001B[32m\u001B[1mSUCCESS \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mvalidate_model\u001B[0m:\u001B[36m34\u001B[0m - \u001B[32m\u001B[1mValidação finalizada. Acurácia média: 0.7100\u001B[0m\n\u001B[32m2025-05-26 11:02:19.044\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mevaluate_model\u001B[0m:\u001B[36m39\u001B[0m - \u001B[1mAvaliando modelo...\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.91      0.94      0.92        31\n           1       0.75      0.67      0.71         9\n\n    accuracy                           0.88        40\n   macro avg       0.83      0.80      0.81        40\nweighted avg       0.87      0.88      0.87        40\n\nAccuracy: 0.8750\nROC AUC: 0.8011\nMatriz de Confusão:\n [[29  2]\n [ 3  6]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-05-26 11:02:20.579\u001B[0m | \u001B[32m\u001B[1mSUCCESS \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36msave_model\u001B[0m:\u001B[36m51\u001B[0m - \u001B[32m\u001B[1mModelo e scaler salvos.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Pipeline de execução: Leitura, treino, validação, avaliação e salvamento\n",
    "\n",
    "# 1. Ler os dados\n",
    "aprendizagem = Sharepoint('General/Arquivo_Databricks/Aprendizagem_fraude_teste.xlsx').read_file(engine = 'openpyxl').to_dataframe()\n",
    "\n",
    "\n",
    "# 2. Pré-processar os dados\n",
    "X, Y, scaler = preprocess_data(aprendizagem)  \n",
    "\n",
    "# 3. Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "# 4. Treinar modelo\n",
    "modelo_rf = train_model(X_train, y_train)\n",
    "\n",
    "# 5. Validar modelo\n",
    "validate_model(modelo_rf, X, Y)\n",
    "\n",
    "# 6. Avaliar modelo no teste\n",
    "evaluate_model(modelo_rf, X_test, y_test)\n",
    "\n",
    "# 7. Salvar modelo e scaler\n",
    "save_model(modelo_rf, scaler)\n",
    "\n",
    "# 8 Teste de melhores hiperparâmetros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a66ce0b-1bd6-43d2-9bc2-8ff0ffec4a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3628\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 3629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3630\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'MES_SEQ'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8677219756460508>, line 27\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(\n",
       "\u001B[1;32m     18\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mrf,\n",
       "\u001B[1;32m     19\u001B[0m     param_grid\u001B[38;5;241m=\u001B[39mparam_grid,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m   \u001B[38;5;66;03m# mostra o progresso\u001B[39;00m\n",
       "\u001B[1;32m     24\u001B[0m )\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Prepara os dados\u001B[39;00m\n",
       "\u001B[0;32m---> 27\u001B[0m X \u001B[38;5;241m=\u001B[39m aprendizagem[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMES_SEQ\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m     28\u001B[0m y \u001B[38;5;241m=\u001B[39m aprendizagem[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUND\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Roda a busca\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n",
       "\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n",
       "\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n",
       "\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/indexes/base.py:3631\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n",
       "\u001B[1;32m   3630\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[0;32m-> 3631\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
       "\u001B[1;32m   3632\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
       "\u001B[1;32m   3633\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n",
       "\u001B[1;32m   3634\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n",
       "\u001B[1;32m   3635\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
       "\u001B[1;32m   3636\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'MES_SEQ'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3628\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3630\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n\n\u001B[0;31mKeyError\u001B[0m: 'MES_SEQ'\n\nThe above exception was the direct cause of the following exception:\n\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-8677219756460508>, line 27\u001B[0m\n\u001B[1;32m     17\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(\n\u001B[1;32m     18\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mrf,\n\u001B[1;32m     19\u001B[0m     param_grid\u001B[38;5;241m=\u001B[39mparam_grid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m   \u001B[38;5;66;03m# mostra o progresso\u001B[39;00m\n\u001B[1;32m     24\u001B[0m )\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Prepara os dados\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m X \u001B[38;5;241m=\u001B[39m aprendizagem[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMES_SEQ\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     28\u001B[0m y \u001B[38;5;241m=\u001B[39m aprendizagem[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUND\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Roda a busca\u001B[39;00m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/indexes/base.py:3631\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3630\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3631\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3632\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3633\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3634\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3635\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3636\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\n\u001B[0;31mKeyError\u001B[0m: 'MES_SEQ'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'MES_SEQ'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 318234398858471,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ML - Análise Fraudes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}