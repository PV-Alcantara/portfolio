{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d74ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas do agente \n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.agents import initialize_agent, AgentType, AgentExecutor # Usado para os sub-agentes\n",
    "from langchain_core.tools import StructuredTool # Para ferramentas com schema\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory # Importação da memória / definição de contexto para armazenamento da memória\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # Para o agente principal\n",
    "from pydantic import BaseModel, Field # Necessário para StructuredTool\n",
    "from langchain_core.messages import HumanMessage, AIMessage \n",
    "\n",
    "\n",
    "#Funções diversas em Python\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "#importações para RAG\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma #banco vetorial\n",
    "import shutil \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7def8e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Documentos carregados. Número de documentos: 1\n",
      "DEBUG: Número de chunks criados: 74\n",
      "DEBUG: Primeiro chunk: DATA,Marca,APRESENTACAO,EAN,MDTR_UND,CPV_UND\n",
      "2024-08-01,LUTEAL (A4H),LUTEAL CAP MOLE BLX60,789665804...\n"
     ]
    }
   ],
   "source": [
    "#1) Criação da pasta onde estão as informações\n",
    "docs_folder = \"docs\"\n",
    "csv_file_path = os.path.join(docs_folder, \"dataset.csv\")\n",
    "\n",
    "#carregar os documentos\n",
    "loader = TextLoader(csv_file_path, encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "print(f\"DEBUG: Documentos carregados. Número de documentos: {len(documents)}\")\n",
    "\n",
    "# Dividir documentos em chunks (pedaços menores)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"DEBUG: Número de chunks criados: {len(splits)}\")\n",
    "print(f\"DEBUG: Primeiro chunk: {splits[0].page_content[:100]}...\")\n",
    "\n",
    "embeddings_model = 'nomic-embed-text'\n",
    "embeddings = OllamaEmbeddings(model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa45df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Removendo diretório existente do ChromaDB para forçar atualização: './chroma_db_agente_local'\n",
      "DEBUG: Diretório do ChromaDB removido com sucesso.\n",
      "DEBUG: Criando e preenchendo novo ChromaDB com documentos atualizados...\n",
      "DEBUG: Novo ChromaDB criado e preenchido com sucesso com dados atualizados!\n",
      "\n",
      "DEBUG: Número de chunks originais (splits): 74\n",
      "DEBUG: Número de documentos no ChromaDB: 74\n",
      "DEBUG: A contagem de documentos no ChromaDB corresponde ao número de splits.\n"
     ]
    }
   ],
   "source": [
    "# 2) Criação ou Carregamento do banco vetorial em chroma_db\n",
    "# Definindo um caminho de pasta específico para este projeto\n",
    "chroma_db_path = \"./chroma_db_agente_local\" \n",
    "vectorstore = None # Inicializa o vectorstore como None\n",
    "\n",
    "if os.path.exists(chroma_db_path):\n",
    "    print(f\"DEBUG: Removendo diretório existente do ChromaDB para forçar atualização: '{chroma_db_path}'\")\n",
    "    shutil.rmtree(chroma_db_path)\n",
    "    print(\"DEBUG: Diretório do ChromaDB removido com sucesso.\")\n",
    "\n",
    "print(\"DEBUG: Criando e preenchendo novo ChromaDB com documentos atualizados...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=chroma_db_path\n",
    ")\n",
    "print(\"DEBUG: Novo ChromaDB criado e preenchido com sucesso com dados atualizados!\")\n",
    "\n",
    "# teste. Contagem de Documentos\n",
    "print(f\"\\nDEBUG: Número de chunks originais (splits): {len(splits)}\")\n",
    "try:\n",
    "    chroma_collection = vectorstore._collection\n",
    "    count_in_db = chroma_collection.count()\n",
    "    print(f\"DEBUG: Número de documentos no ChromaDB: {count_in_db}\")\n",
    "\n",
    "    if count_in_db == len(splits):\n",
    "        print(\"DEBUG: A contagem de documentos no ChromaDB corresponde ao número de splits.\")\n",
    "    else:\n",
    "        print(\"ALERTA: A contagem de documentos no ChromaDB NÃO corresponde ao número de splits. Pode haver um problema.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Não foi possível contar documentos no ChromaDB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97944064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Dataset CSV carregado em DataFrame. Linhas: 765\n"
     ]
    }
   ],
   "source": [
    "#3) Criação de funções\n",
    "\n",
    "#criação do dataset\n",
    "df_info_dataset = None\n",
    "try:\n",
    "    df_info_dataset = pd.read_csv(csv_file_path)\n",
    "    print(f\"DEBUG: Dataset CSV carregado em DataFrame. Linhas: {len(df_info_dataset)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: O arquivo '{csv_file_path}' não foi encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Não foi possível carregar o CSV para DataFrame: {e}\")\n",
    "\n",
    "\n",
    "def buscar_documentos (query: str) -> str:\n",
    "    \"\"\"Use esta ferramenta para **buscar informações textuais relevantes** do dataset armazenado no chroma_db_agente_local.\n",
    "    Esta ferramenta aceita uma 'query' em **linguagem natural** que descreve a informação que você precisa,\n",
    "    como 'detalhes sobre o produto X' ou 'o que é mencionado sobre a Marca Y'.\n",
    "    Ela **retorna o conteúdo relevante dos documentos em formato de texto**.\n",
    "    **Não a utilize para obter dados numéricos ou tabulares para cálculos ou agregações.**\n",
    "    Exemplo de uso: buscar_documentos(query=\"descrição da marca LUTEAL\")\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    retriever = vectorstore.as_retriever()\n",
    "    docs = retriever.invoke(query)\n",
    "    if docs:\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    else:\n",
    "        return \"Não encontrei informações relevantes nos documentos para esta consulta.\"\n",
    "    \n",
    "def sql_documentos (query_sql: str) -> str:\n",
    "    \"\"\"Use esta ferramenta para **consultas quantitativas/qualitativas** dentro do dataset chamado 'df_info_dataset'\n",
    "    Esta ferramenta exige uma query onde o 'from' deve ser sempre 'df_info_dataset\n",
    "    exemplo1 soma: \n",
    "    select max(Data), Marca, sum(CPV_UND) \n",
    "    from df_info_dataset\n",
    "    group by max(Data), Marca\n",
    "\n",
    "    exemplo2 conversão:\n",
    "    select sum(CPV_UND) / sum(MDTR_UND)\n",
    "    from df_info_dataset\n",
    "    group by Marca\n",
    "\n",
    "    exemplo3 conversão:\n",
    "    select sum(CPV_UND) / sum(MDTR_UND)\n",
    "    from df_info_dataset\n",
    "    where Marca = 'MOTORE (A4H)'\n",
    "    group by Marca\n",
    "    \"\"\"\n",
    "    \n",
    "    global df_info_dataset # Permite que a função acesse o df_data global\n",
    "    if df_info_dataset is None:\n",
    "        return \"Erro: O DataFrame 'df_info_dataset' não foi carregado. Não é possível executar consultas SQL.\"\n",
    "\n",
    "    colunas_originais = df_info_dataset.columns.tolist()\n",
    "    \n",
    "    # Cria dicionário de mapeamento simples\n",
    "    mapeamento = {}\n",
    "    for col in colunas_originais:\n",
    "        col_nova = col.upper()\n",
    "        col_nova = re.sub(r\"[^A-Z0-9]\", \"_\", col_nova)  # Substitui espaços/símbolos por _\n",
    "        col_nova = re.sub(r\"_+\", \"_\", col_nova).strip(\"_\")  # Remove múltiplos _\n",
    "        mapeamento[col] = col_nova\n",
    "\n",
    "    # Aplica o mapeamento\n",
    "    df_info_dataset.columns = [mapeamento[col] for col in df_info_dataset.columns]\n",
    "    \n",
    "\n",
    "    result = sqldf(query_sql, {'df_info_dataset': df_info_dataset}) # Mapeamento do dataset para o agente\n",
    "\n",
    "   \n",
    "    if not result.empty and len(result.columns) == 1 and len(result) == 1:\n",
    "        return str(result.iloc[0, 0]) # Retorna o valor da única célula como string\n",
    "    else:\n",
    "        return result.to_string(index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6866b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Criação de Schema\n",
    "class buscar_documentos_input(BaseModel):\n",
    "    query: str = Field(description=\"Consulta na documentaçao.\")\n",
    "\n",
    "class sql_documentos_input(BaseModel):\n",
    "    query_sql: str = Field(description=\"Consulta SQL no dataset df_info_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828cb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Criação das ferramentas estruturadas\n",
    "buscar_tool = StructuredTool.from_function(\n",
    "    func=buscar_documentos,\n",
    "    name='buscar_documentos',\n",
    "    description=buscar_documentos.__doc__.strip(),\n",
    "    args_schema=buscar_documentos_input\n",
    ")\n",
    "\n",
    "query_tool = StructuredTool.from_function(\n",
    "    func=sql_documentos,\n",
    "    name='sql_documentos',\n",
    "    description=sql_documentos.__doc__.strip(),\n",
    "    args_schema=sql_documentos_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d622973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_14664\\2064540434.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  main_memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "#6) Criação da memória dos agentes\n",
    "main_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    k=5 # Mantenha as últimas 5 interações (5 pares de HumanMessage/AIMessage)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c9da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Agente configurado com sucesso para LLM local.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Temp\\ipykernel_14664\\3880011911.py:7: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "#7) Criação do Agente\n",
    "llm = OllamaLLM(model=\"qwen3\",\n",
    "                temperature=0.2,\n",
    "                max_tokens=500              \n",
    "                )\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools=[query_tool, buscar_tool],\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=main_memory, # Adicionando memória\n",
    "    agent_kwargs={\"system_message\": \"\"\"Você é um analista de dados focado em trazer os resultados de projetos.\n",
    "    Sempre utilize as ferramentas disponíveis para responder às perguntas do usuário.\n",
    "\n",
    "    **IMPORTANTE: Seu processo de pensamento deve seguir o formato ReAct (Thought, Action, Action Input, Observation, Thought, Final Answer).**\n",
    "\n",
    "    **Regras de Uso das Ferramentas:**\n",
    "    1.  **Para perguntas de caráter quantitativo ou qualitativo que exigem análise de dados tabulares, agregação (COUNT, SUM, AVG), filtragem precisa ou qualquer cálculo sobre o dataset, VOCÊ DEVE usar a ferramenta 'query_tool'. Lembre-se que o nome da tabela no SQL é 'df_info_dataset'.**\n",
    "        * Exemplo de Thought para esta ferramenta: \"A pergunta é quantitativa, preciso usar query_tool para contar as marcas.\"\n",
    "        * Exemplo de Final Answer após usar esta ferramenta: \"Existem X marcas no dataset.\"\n",
    "        * **EXEMPLO DE USO PARA CÁLCULO DE CONVERSÃO (use este padrão!):** Para \"Me diga a conversão da Marca Artrolive em maio/2024\", a query SQL seria:\n",
    "          `SELECT SUM(CPV_UND) * 1.0 / SUM(MDTR_UND) FROM df_info_dataset WHERE Marca = 'ARTROLIVE (A4H)' AND strftime('%Y-%m', DATA) = '2024-05'`\n",
    "          **Lembre-se de converter para percentual no Final Answer.**\n",
    "                  \n",
    "    2.  **Para perguntas sobre informações descritivas que precisam de uma interpretação de texto, ou para buscar trechos de texto específicos em documentos, use a ferramenta 'buscar_tool'.**\n",
    "        * Exemplo de Thought para esta ferramenta: \"A pergunta é descritiva, preciso buscar informações no texto dos documentos.\"\n",
    "\n",
    "    **Estrutura do Dataset 'df_info_dataset' (Colunas e Tipos Principais):**\n",
    "    * `DATA`: Tipo data (yyyy-mm-dd).\n",
    "    * `MARCA`: Tipo texto (ex: 'LUTEAL (A4H)').\n",
    "    * `APRESENTACAO`: Tipo texto (ex: 'LUTEAL CAP MOLE BLX60').\n",
    "    * `EAN`: Tipo texto/numérico (código, ex: '7896658040894').\n",
    "    * `MDTR_UND`: Tipo numérico (Unidades de MDTR).\n",
    "    * `CPV_UND`: Tipo numérico (Unidades de CPV).\n",
    "\n",
    "    **Sinônimos e Definições Adicionais:**\n",
    "    * Marca pode ser também 'Produto'.\n",
    "    * Data pode ser também 'Período, Data ou Ocorrência'.\n",
    "    * Apresentacao pode ser também 'SKU'.\n",
    "    * EAN pode ser também 'Código'.\n",
    "    * CPV_UND pode ser também 'UNIDADES CPV','CPV_UNIDADES','UND_CAMPANHA','UNIDADES CAMPANHA'.\n",
    "    * Conversão é o cálculo: `sum(CPV_UND) / sum(MDTR_UND)`. O resultado é em percentual.\n",
    "    * Caso o usuário já traga o nome da Marca, não se esqueça de adicionar o sulfixo ' (A4H)' caso ele não tenha escrito.\n",
    "\n",
    "    * Se uma pergunta não se encaixar em nenhuma ferramenta, responda diretamente.\n",
    "    * **Sempre responda de forma resumida e coerente ao que foi perguntado, focando na resposta final APÓS usar e interpretar o resultado da ferramenta.**\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    ")\n",
    "print(\"DEBUG: Agente configurado com sucesso para LLM local.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de7357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Teste 1: Pergunta 1 ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user is asking how many brands exist in the file. The dataset is called 'df_info_dataset'. To find the number of unique brands, I should use the sql_documentos tool. The SQL query needs to count distinct entries in the 'Marca' column. Let me check the syntax. The correct query would be SELECT COUNT(DISTINCT Marca) FROM df_info_dataset. That should give the number of unique brands. I don't need to use the other tool since it's a quantitative question. Let me execute that.\n",
      "</think>\n",
      "\n",
      "Thought: Preciso determinar o número de marcas únicas no dataset. Para isso, devo usar a ferramenta sql_documentos com uma consulta que conte as marcas distintas.\n",
      "\n",
      "Action: sql_documentos\n",
      "Action Input: SELECT COUNT(DISTINCT Marca) FROM df_info_dataset\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m35\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user asked how many brands exist in the file. I used the sql_documentos tool to run the query SELECT COUNT(DISTINCT Marca) FROM df_info_dataset. The result was 35. That means there are 35 unique brands in the dataset. I don't need to use the other tool because the question is about counting, not textual information. The answer is straightforward from the SQL result.\n",
      "</think>\n",
      "\n",
      "Final Answer: Existem 35 marcas no arquivo.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: Existem 35 marcas no arquivo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8) Testando o agente\n",
    "chat_history = []\n",
    "\n",
    "print(\"\\n--- Teste 1: Pergunta 1 ---\")\n",
    "user_input_1 = \"Quantas Marcas existem neste arquivo?\"\n",
    "response_1 = agent_executor.invoke({\"input\": user_input_1, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_1), AIMessage(content=response_1[\"output\"])])\n",
    "print(f\"Resposta: {response_1['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd5808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Teste 2: Pergunta de cálculo ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, let's tackle this question. The user wants to know which brand had the highest sales in terms of total CPV units. \n",
      "\n",
      "First, I need to understand what data I have. The dataset is called df_info_dataset, and it includes columns like CPV_UND, which I assume represents the number of units sold for each brand. The goal is to find the brand with the maximum sum of CPV_UND.\n",
      "\n",
      "Since the question is about aggregating numerical data (summing CPV_UND per brand and finding the maximum), the sql_documentos tool is the right choice here. The tool requires a SQL query that groups by brand and sums CPV_UND, then selects the brand with the highest value.\n",
      "\n",
      "I should structure the SQL query to group by the brand column (probably named 'Marca') and calculate the sum of CPV_UND for each. Then, order the results in descending order and limit to the top result. That should give the brand with the highest total units.\n",
      "\n",
      "Wait, the example given in the tool's description uses 'group by max(Data), Marca', but that might not be correct. Wait, no, the example is for a different purpose. Let me focus on the correct approach. The correct query would be to group by Marca, sum CPV_UND, then order by that sum in descending order and take the first row.\n",
      "\n",
      "So the SQL query would be: SELECT Marca, SUM(CPV_UND) AS Total_CPV FROM df_info_dataset GROUP BY Marca ORDER BY Total_CPV DESC LIMIT 1;\n",
      "\n",
      "But I need to make sure that the SQL syntax is correct. Also, the tool's examples show using group by with aggregate functions. Let me check the example again. The first example groups by max(Data) and Marca, but that's probably a typo. The correct approach is to group by Marca and aggregate CPV_UND.\n",
      "\n",
      "So, the correct query should be:\n",
      "\n",
      "SELECT Marca, SUM(CPV_UND) AS Total_CPV\n",
      "FROM df_info_dataset\n",
      "GROUP BY Marca\n",
      "ORDER BY Total_CPV DESC\n",
      "LIMIT 1;\n",
      "\n",
      "This will give the brand with the highest total CPV units. Then, the sql_documentos tool will execute this query and return the result. The answer will be the brand name from that result.\n",
      "</think>\n",
      "\n",
      "Thought: Preciso encontrar a marca com o maior total de unidades CPV. Isso requer uma consulta que some as unidades CPV por marca e identifique a marca com o valor máximo.\n",
      "\n",
      "Action: sql_documentos\n",
      "Action Input: \n",
      "SELECT Marca, SUM(CPV_UND) AS Total_CPV\n",
      "FROM df_info_dataset\n",
      "GROUP BY Marca\n",
      "ORDER BY Total_CPV DESC\n",
      "LIMIT 1;\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m          MARCA  Total_CPV\n",
      "ARTROLIVE (A4H)     541074\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Final Answer: A marca que mais vendeu, considerando o total de unidades CPV, foi a ARTROLIVE (A4H) com um total de 541.074 unidades.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: A marca que mais vendeu, considerando o total de unidades CPV, foi a ARTROLIVE (A4H) com um total de 541.074 unidades.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Teste 2: Pergunta de cálculo ---\")\n",
    "user_input_2 = \"Considerando o total de unidades CPV, qual foi a marca que mais vendeu?\"\n",
    "response_2 = agent_executor.invoke({\"input\": user_input_2, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_2), AIMessage(content=response_2[\"output\"])])\n",
    "print(f\"Resposta: {response_2['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "074e4bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Teste 3: Pergunta sobre documentos ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, I need to find the third largest brand sold in May 2024. Let me start by figuring out how to approach this.\n",
      "\n",
      "First, I should check the dataset 'df_info_dataset' for sales data. The user mentioned using sql_documentos for quantitative queries. So, I need to write an SQL query that sums the sales (probably CPV_UND) for each brand in May 2024. Then, order them in descending order and pick the third one.\n",
      "\n",
      "Wait, the SQL function requires the 'from' clause to be 'df_info_dataset'. Also, the example uses grouping with max(Data), but maybe I should just group by Marca and sum CPV_UND. Let me structure the query correctly.\n",
      "\n",
      "I need to filter the data for May 2024. Assuming the 'Data' column is a date, I should use a WHERE clause to select records where Data is between '2024-05-01' and '2024-05-31'. Then group by Marca, sum CPV_UND, order by the sum in descending order, and limit to the top 3 to find the third one.\n",
      "\n",
      "Wait, but the example shows grouping by max(Data), which might not be necessary. Maybe the Data column is already the date, so I can just filter for May 2024. Let me try writing the query:\n",
      "\n",
      "SELECT Marca, SUM(CPV_UND) AS TotalVendas\n",
      "FROM df_info_dataset\n",
      "WHERE Data BETWEEN '2024-05-01' AND '2024-05-31'\n",
      "GROUP BY Marca\n",
      "ORDER BY TotalVendas DESC\n",
      "LIMIT 3;\n",
      "\n",
      "But the user's example uses 'group by max(Data), Marca', which might be a typo. Maybe they meant to group by Marca and use max(Data) as part of the selection. However, for this question, I think the correct approach is to group by Marca and sum CPV_UND, then order by the sum. \n",
      "\n",
      "I should execute this query using sql_documentos. Let me check if the Data column is in the correct format. If the Data is stored as a string, maybe I need to adjust the date format. But assuming it's a date type, the BETWEEN clause should work. \n",
      "\n",
      "Once I get the top 3 brands, the third one in the ordered list would be the answer. Let me proceed with the SQL query.\n",
      "</think>\n",
      "\n",
      "Thought: Preciso encontrar a terceira maior marca em termos de vendas em maio de 2024. Para isso, devo calcular o total de vendas por marca no mês de maio de 2024, ordenar as marcas por esse total em ordem descrescente e identificar a terceira posição.\n",
      "\n",
      "Action: sql_documentos\n",
      "Action Input: \n",
      "SELECT Marca, SUM(CPV_UND) AS TotalVendas\n",
      "FROM df_info_dataset\n",
      "WHERE Data BETWEEN '2024-05-01' AND '2024-05-31'\n",
      "GROUP BY Marca\n",
      "ORDER BY TotalVendas DESC\n",
      "LIMIT 3;\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m          MARCA  TotalVendas\n",
      "ARTROLIVE (A4H)        46811\n",
      "  ZOLFEST (A4H)        14837\n",
      "   MOTORE (A4H)        14655\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Final Answer: A terceira maior marca vendida em maio de 2024 foi MOTORE (A4H).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: A terceira maior marca vendida em maio de 2024 foi MOTORE (A4H).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Teste 3: Pergunta sobre documentos ---\")\n",
    "user_input_3 = \"Qual a terceira maior marca vendido em maio de 2024?\"\n",
    "response_3 = agent_executor.invoke({\"input\": user_input_3, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_3), AIMessage(content=response_3[\"output\"])])\n",
    "print(f\"Resposta: {response_3['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "888409b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Teste 4: Outra pergunta sobre documentos ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user is asking for the conversion of the \"Motore\" brand in June 2024. Let me break this down. First, I need to understand what \"conversão\" means here. In the context of the provided tools, the examples show that conversion refers to the ratio of CPV_UND to MDTR_UND. So, the user wants the sum of CPV_UND divided by the sum of MDTR_UND for the Motore brand in June 2024.\n",
      "\n",
      "I should use the sql_documentos tool since it's a quantitative calculation. The query needs to select the sum of CPV_UND divided by sum of MDTR_UND, filtered by Marca = 'MOTORE (A4H)' and Data in June 2024. Wait, the example uses 'group by Marca', but since we're already filtering by Marca, maybe the group by isn't necessary. But to be safe, perhaps include it. Also, the date format in the dataset might be important. The example uses 'max(Data)', but the user specified June 2024. So the where clause should specify Data = '2024-06' or similar. Let me check the examples again. The second example has 'where Marca = 'MOTORE (A4H)' and groups by Marca. So I should structure the query to filter by the brand and the date, then compute the ratio. Let me write the SQL query accordingly.\n",
      "</think>\n",
      "\n",
      "Thought: Preciso calcular a conversão da marca \"MOTORE (A4H)\" em junho de 2024, que é a soma de CPV_UND dividida pela soma de MDTR_UND. Devo usar a ferramenta sql_documentos com uma query específica.\n",
      "\n",
      "Action: sql_documentos\n",
      "Action Input: \n",
      "select sum(CPV_UND) / sum(MDTR_UND)\n",
      "from df_info_dataset\n",
      "where Marca = 'MOTORE (A4H)' and Data between '2024-06-01' and '2024-06-30'\n",
      "group by Marca\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m0.7197845636920644\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m<think>\n",
      "</think>\n",
      "\n",
      "Final Answer: A conversão da marca Motore em junho de 2024 é aproximadamente 0.72.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta: A conversão da marca Motore em junho de 2024 é aproximadamente 0.72.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Teste 4: Outra pergunta sobre documentos ---\")\n",
    "user_input_4 = \"Me diga a conversão da Marca Motore em junho de 2024\"\n",
    "response_4 = agent_executor.invoke({\"input\": user_input_4, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=user_input_4), AIMessage(content=response_4[\"output\"])])\n",
    "print(f\"Resposta: {response_4['output']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
