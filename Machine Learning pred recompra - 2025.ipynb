{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b31894-a276-48a2-9b08-e4f21bb17541",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Criação Base"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view base_pred as (\n",
    "    with base as (\n",
    "        select\n",
    "        fvend.ID_PORTADOR as ID_PORTADOR\n",
    "        ,dpdv.UF as UF_PORTADOR\n",
    "        ,dpdv.GRUPO as GRUPO\n",
    "        ,sum(fvend.QUANTIDADE) as QTD\n",
    "        ,count(distinct fvend.ID_VENDA) as QTD_TRANSACOES\n",
    "        ,max(fvend.QUANTIDADE) as MAIOR_QTD\n",
    "        ,count(distinct fvend.ID_MARCA) as QTD_MARCAS\n",
    "    \n",
    "        from \n",
    "            dmn_transformacao_digital_dev.db_analytics.cpv_fvendas as fvend\n",
    "            left join dmn_transformacao_digital_dev.db_analytics.cpv_dprodutos as dprod on fvend.ID_PRODUTO = dprod.ID_PRODUTO\n",
    "            left join dmn_transformacao_digital_dev.db_analytics.cpv_dpdv as dpdv on dpdv.ID_LOJA = fvend.ID_LOJA  \n",
    "            left join dmn_transformacao_digital_dev.db_analytics.cpv_dportador as dport on dport.ID_PORTADOR = fvend.ID_PORTADOR\n",
    "\n",
    "        where \n",
    "            fvend.DATA_COMPRA >= '2025-01-01' and fvend.DATA_COMPRA <= '2025-04-30'\n",
    "            AND dprod.MARCA = 'ALENIA (A4H)'\n",
    "        group by all\n",
    "        ),\n",
    "\n",
    "        classificacao as (\n",
    "            select\n",
    "            fvend.ID_PORTADOR as ID_PORTADOR\n",
    "            ,dpdv.GRUPO as GRUPO\n",
    "            \n",
    "            from \n",
    "            dmn_transformacao_digital_dev.db_analytics.cpv_fvendas as fvend\n",
    "            left join dmn_transformacao_digital_dev.db_analytics.cpv_dprodutos as dprod on fvend.ID_PRODUTO = dprod.ID_PRODUTO\n",
    "            left join dmn_transformacao_digital_dev.db_analytics.cpv_dpdv as dpdv on dpdv.ID_LOJA = fvend.ID_LOJA  \n",
    "            left join dmn_transformacao_digital_dev.db_analytics.cpv_dportador as dport on dport.ID_PORTADOR = fvend.ID_PORTADOR\n",
    "\n",
    "        where \n",
    "            fvend.DATA_COMPRA >= '2025-05-01' and fvend.DATA_COMPRA <= '2025-05-31'\n",
    "            AND dprod.MARCA = 'ALENIA (A4H)'\n",
    "        group by fvend.ID_PORTADOR, dpdv.GRUPO),\n",
    "\n",
    "        agrupadores as (\n",
    "            SELECT\n",
    "                ID_PORTADOR,\n",
    "                AVG(dias_desde_anterior) as MED_DIAS_ENTRE_COMPRAS\n",
    "            FROM (\n",
    "                SELECT\n",
    "                    fvend.ID_PORTADOR,\n",
    "                    fvend.DATA_COMPRA,\n",
    "                    LAG(fvend.DATA_COMPRA, 1, NULL) OVER (PARTITION BY fvend.ID_PORTADOR ORDER BY fvend.DATA_COMPRA ASC) as DATA_COMPRA_ANTERIOR,\n",
    "                    DATEDIFF(DATA_COMPRA, DATA_COMPRA_ANTERIOR) as dias_desde_anterior\n",
    "                FROM\n",
    "                    dmn_transformacao_digital_dev.db_analytics.cpv_fvendas as fvend\n",
    "                    left join dmn_transformacao_digital_dev.db_analytics.cpv_dprodutos as dprod on fvend.ID_PRODUTO = dprod.ID_PRODUTO\n",
    "                    left join dmn_transformacao_digital_dev.db_analytics.cpv_dpdv as dpdv on dpdv.ID_LOJA = fvend.ID_LOJA  \n",
    "                    left join dmn_transformacao_digital_dev.db_analytics.cpv_dportador as dport on dport.ID_PORTADOR = fvend.ID_PORTADOR\n",
    "                WHERE\n",
    "                    fvend.DATA_COMPRA >= '2025-01-01' and fvend.DATA_COMPRA <= '2025-04-30'\n",
    "                    AND dprod.MARCA = 'ALENIA (A4H)' \n",
    "                ) as vendas_com_data_anterior\n",
    "            WHERE DATA_COMPRA_ANTERIOR IS NOT NULL -- Exclui a primeira compra de cada portador, que não tem anterior\n",
    "            GROUP BY ID_PORTADOR\n",
    "            ) \n",
    "\n",
    "select \n",
    "base.*\n",
    ",agrupadores.MED_DIAS_ENTRE_COMPRAS\n",
    ", CASE WHEN isnull(classificacao.ID_PORTADOR) THEN 0 ELSE 1 END as CLASSE\n",
    "\n",
    "from base as base\n",
    "left join classificacao on classificacao.id_portador = base.id_portador and base.grupo = classificacao.grupo\n",
    "left join agrupadores on agrupadores.id_portador = base.id_portador\n",
    "\n",
    "group by all\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "904848e3-c212-4e35-85f8-f724d1e51757",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instalação imblearn (smote)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c8b780c3-bf02-4c43-b646-4dfdd3595969/lib/python3.11/site-packages (0.0)\nRequirement already satisfied: imbalanced-learn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c8b780c3-bf02-4c43-b646-4dfdd3595969/lib/python3.11/site-packages (from imblearn) (0.13.0)\nRequirement already satisfied: numpy<3,>=1.24.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c8b780c3-bf02-4c43-b646-4dfdd3595969/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.26.4)\nRequirement already satisfied: scipy<2,>=1.10.1 in /databricks/python3/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.11.1)\nRequirement already satisfied: scikit-learn<2,>=1.3.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c8b780c3-bf02-4c43-b646-4dfdd3595969/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.6.1)\nRequirement already satisfied: sklearn-compat<1,>=0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c8b780c3-bf02-4c43-b646-4dfdd3595969/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (0.1.3)\nRequirement already satisfied: joblib<2,>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.2.0)\nRequirement already satisfied: threadpoolctl<4,>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c8b780c3-bf02-4c43-b646-4dfdd3595969/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (3.6.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559836a5-3f96-4aeb-9286-ab03e91c8e12",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1. Carregamento dos Dados"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Carregando dados da view 'base_pred' para um DataFrame Pandas...\n   Dados carregados. Shape: (499351, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Usado para np.nan, útil para simulações ou verificações\n",
    "\n",
    "# Importações do Scikit-learn para pré-processamento e modelo\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer # Importante para aplicar diferentes transformações a diferentes colunas\n",
    "\n",
    "# Importações do Imbalanced-learn para lidar com desbalanceamento\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Pipeline que integra balanceamento\n",
    "\n",
    "# Importações para avaliação do modelo\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# --- 1. Carregamento dos Dados ---\n",
    "# Assume que 'spark' (o objeto SparkSession) está disponível no seu ambiente\n",
    "# e que a view temporária 'base_pred' foi criada com sua query SQL.\n",
    "print(\"1. Carregando dados da view 'base_pred' para um DataFrame Pandas...\")\n",
    "pd_base_pred = spark.sql(\"select * from base_pred\").toPandas()\n",
    "print(f\"   Dados carregados. Shape: {pd_base_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d60358e-4a5e-45e6-95d8-43d416f0aaaf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2. Separar Features (X) e Target (y)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. Colunas de features e target definidas.\n   Shape de X (features): (499351, 8)\n   Shape de y (target): (499351,)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Separar Features (X) e Target (y) ---\n",
    "# Define as colunas que serão usadas como features e a coluna target (CLASSE)\n",
    "features_cols = [\n",
    "    'ID_PORTADOR',\n",
    "    'UF_PORTADOR',\n",
    "    'GRUPO',\n",
    "    'QTD',\n",
    "    'QTD_TRANSACOES',\n",
    "    'MAIOR_QTD',\n",
    "    'QTD_MARCAS',\n",
    "    'MED_DIAS_ENTRE_COMPRAS'\n",
    "]\n",
    "target_col = 'CLASSE'\n",
    "\n",
    "pd_base_pred_x = pd_base_pred[features_cols].copy() # .copy() para evitar SettingWithCopyWarning\n",
    "pd_base_pred_y = pd_base_pred[target_col].copy()\n",
    "\n",
    "print(\"\\n2. Colunas de features e target definidas.\")\n",
    "print(f\"   Shape de X (features): {pd_base_pred_x.shape}\")\n",
    "print(f\"   Shape de y (target): {pd_base_pred_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60365b63-1720-42dd-b315-a2b310d253b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3. Tratamento de Nulos"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3. Verificando e tratando valores nulos...\n   Nulos antes do tratamento:\nID_PORTADOR                    0\nUF_PORTADOR                    0\nGRUPO                          0\nQTD                            0\nQTD_TRANSACOES                 0\nMAIOR_QTD                      0\nQTD_MARCAS                     0\nMED_DIAS_ENTRE_COMPRAS    255471\ndtype: int64\n\n   Nulos após o tratamento:\nID_PORTADOR               0\nUF_PORTADOR               0\nGRUPO                     0\nQTD                       0\nQTD_TRANSACOES            0\nMAIOR_QTD                 0\nQTD_MARCAS                0\nMED_DIAS_ENTRE_COMPRAS    0\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Tratamento de Nulos ---\n",
    "print(\"\\n3. Verificando e tratando valores nulos...\")\n",
    "print(\"   Nulos antes do tratamento:\")\n",
    "print(pd_base_pred_x.isnull().sum())\n",
    "\n",
    "# 3.1. Preencher Nulos em Colunas Numéricas com Zero\n",
    "# Para features numéricas de compra, NULL geralmente significa ausência de atividade.\n",
    "colunas_numericas_para_zero = [\n",
    "    'QTD',\n",
    "    'QTD_TRANSACOES',\n",
    "    'MAIOR_QTD',\n",
    "    'QTD_MARCAS',\n",
    "    'MED_DIAS_ENTRE_COMPRAS'\n",
    "]\n",
    "\n",
    "for col in colunas_numericas_para_zero:\n",
    "    if col in pd_base_pred_x.columns:\n",
    "        pd_base_pred_x[col] = pd_base_pred_x[col].fillna(0)\n",
    "\n",
    "# 3.2. Preencher Nulos em Colunas Categóricas com 'DESCONHECIDO'\n",
    "# Para colunas categóricas, um nulo deve ser uma categoria própria, não zero.\n",
    "colunas_categoricas = ['UF_PORTADOR', 'GRUPO']\n",
    "\n",
    "for col in colunas_categoricas:\n",
    "    if col in pd_base_pred_x.columns:\n",
    "        pd_base_pred_x[col] = pd_base_pred_x[col].fillna('DESCONHECIDO')\n",
    "\n",
    "print(\"\\n   Nulos após o tratamento:\")\n",
    "print(pd_base_pred_x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cda8993-978e-41f9-8b67-858f54b656fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Codificação de Variáveis Categóricas"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n4. Codificando variáveis categóricas com LabelEncoder...\n   Coluna 'UF_PORTADOR' codificada para: [ 4 25 10 22 17 14 18 15  2 23 16 12  7  9  8 19  5 13  6  1 26 11  3  0\n 20 24 21]\n   Coluna 'GRUPO' codificada para: [ 4 15  6 24 17 14  9 19 16 11  2 21  3 28  8  1 25 10  7 13  0 22 20  5\n 26 23 12 27 18]\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Codificação de Variáveis Categóricas (LabelEncoder) ---\n",
    "# Aplica LabelEncoder para converter categorias (strings) em números inteiros.\n",
    "# Isso deve ser feito ANTES do ColumnTransformer, pois ColumnTransformer espera entradas numéricas.\n",
    "print(\"\\n4. Codificando variáveis categóricas com LabelEncoder...\")\n",
    "for col in colunas_categoricas:\n",
    "    lencoder = LabelEncoder()\n",
    "    pd_base_pred_x[col] = lencoder.fit_transform(pd_base_pred_x[col])\n",
    "    print(f\"   Coluna '{col}' codificada para: {pd_base_pred_x[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad933a0-b2d0-4929-a8cd-99bdc6e678c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Remover ID_PORTADOR das Features"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n5. 'ID_PORTADOR' removido das features. Colunas finais para o modelo: ['UF_PORTADOR', 'GRUPO', 'QTD', 'QTD_TRANSACOES', 'MAIOR_QTD', 'QTD_MARCAS', 'MED_DIAS_ENTRE_COMPRAS']\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Remover ID_PORTADOR das Features ---\n",
    "# ID_PORTADOR é um identificador e não deve ser usado como feature preditiva.\n",
    "X_features = pd_base_pred_x.drop(columns=['ID_PORTADOR'])\n",
    "print(f\"\\n5. 'ID_PORTADOR' removido das features. Colunas finais para o modelo: {X_features.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516a1d49-112b-46fc-a81c-fd038d797398",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Divisão dos Dados em Treino e Teste"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n6. Dividindo os dados em conjuntos de treino e teste...\n   Shape dos dados de treino: X_train=(399480, 7), y_train=(399480,)\n   Shape dos dados de teste: X_test=(99871, 7), y_test=(99871,)\n   Proporção da Classe 1 no treino: 0.26\n   Proporção da Classe 1 no teste: 0.26\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Divisão dos Dados em Treino e Teste ---\n",
    "# Divide os dados em conjuntos de treino e teste (80/20).\n",
    "# 'stratify=pd_base_pred_y' garante que a proporção das classes seja mantida em ambos os conjuntos.\n",
    "# 'random_state' garante a reprodutibilidade dos resultados.\n",
    "print(\"\\n6. Dividindo os dados em conjuntos de treino e teste...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, pd_base_pred_y,\n",
    "                                                    test_size=0.2, random_state=42,\n",
    "                                                    stratify=pd_base_pred_y)\n",
    "\n",
    "print(f\"   Shape dos dados de treino: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"   Shape dos dados de teste: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "print(f\"   Proporção da Classe 1 no treino: {y_train.value_counts(normalize=True)[1]:.2f}\")\n",
    "print(f\"   Proporção da Classe 1 no teste: {y_test.value_counts(normalize=True)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e777235-4898-4420-895b-9bef8e621e46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definição do Pipeline e Parâmetros para GridSearchCV"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n8. Iniciando o treinamento do modelo...\nTreinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Definição do Pipeline e Parâmetros para GridSearchCV ---\n",
    "\n",
    "# 7.1. Identifica as colunas para o ColumnTransformer\n",
    "# Colunas numéricas que precisam ser escaladas (já tratadas para nulos)\n",
    "numerical_features = [\n",
    "    'QTD', 'QTD_TRANSACOES', 'MAIOR_QTD', 'QTD_MARCAS', 'MED_DIAS_ENTRE_COMPRAS'\n",
    "]\n",
    "# Colunas categóricas que já foram LabelEncoded e devem ser passadas sem escalonamento\n",
    "categorical_features_encoded = ['UF_PORTADOR', 'GRUPO']\n",
    "\n",
    "# 7.2. Cria o ColumnTransformer\n",
    "# 'StandardScaler' será aplicado apenas às 'numerical_features'.\n",
    "# 'passthrough' será aplicado às 'categorical_features_encoded', ou seja, elas não serão alteradas.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_scaler', StandardScaler(), numerical_features), # Nome 'num_scaler' para o passo de escalonamento numérico\n",
    "        ('cat_passthrough', 'passthrough', categorical_features_encoded) # Nome 'cat_passthrough' para o passo de categorias\n",
    "    ])\n",
    "\n",
    "# 7.3. Define o Pipeline completo\n",
    "# Inclui o pré-processador, SMOTE e o classificador.\n",
    "# O 'ImbPipeline' é crucial para que o SMOTE seja aplicado corretamente dentro do K-Fold.\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor), # O primeiro passo é o ColumnTransformer para pré-processamento\n",
    "    ('smote', SMOTE(random_state=42)), # O segundo passo é o balanceamento com SMOTE\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced')) # O último passo é o classificador\n",
    "])\n",
    "\n",
    "# 8. Treinamento do Modelo\n",
    "print(\"\\n8. Iniciando o treinamento do modelo...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Treinamento concluído.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b039b705-54e1-4a4e-bc3b-ce7f713a9850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nRelatório de Classificação no conjunto de teste:\n              precision    recall  f1-score   support\n\n           0       0.82      0.82      0.82     73507\n           1       0.50      0.51      0.50     26364\n\n    accuracy                           0.74     99871\n   macro avg       0.66      0.66      0.66     99871\nweighted avg       0.74      0.74      0.74     99871\n\n\nMatriz de Confusão:\n[[60030 13477]\n [12982 13382]]\n\nAcurácia do modelo no conjunto de teste: 0.7351\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9. Avaliação do Modelo\n",
    "# Faz previsões no conjunto de teste\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# 9.1. Imprimir as métricas de avaliação\n",
    "print(\"\\nRelatório de Classificação no conjunto de teste:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 9.2. Imprimir a acurácia\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAcurácia do modelo no conjunto de teste: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5823825202347591,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Machine Learning pred recompra - 2025",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}