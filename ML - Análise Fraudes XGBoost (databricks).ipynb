{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f135e078-d490-4681-a57c-d4c251575e1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instalação de App"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Using the pkg_resources metadata backend is deprecated. pip 26.3 will enforce this behaviour change. A possible replacement is to use the default importlib.metadata backend, by unsetting the _PIP_USE_IMPORTLIB_METADATA environment variable. Discussion can be found at https://github.com/pypa/pip/issues/13317\u001B[0m\u001B[33m\n\u001B[0mRequirement already satisfied: pip in /local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages (25.1.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[33mDEPRECATION: Using the pkg_resources metadata backend is deprecated. pip 26.3 will enforce this behaviour change. A possible replacement is to use the default importlib.metadata backend, by unsetting the _PIP_USE_IMPORTLIB_METADATA environment variable. Discussion can be found at https://github.com/pypa/pip/issues/13317\u001B[0m\u001B[33m\n\u001B[0mRequirement already satisfied: python-slugify in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (8.0.4)\nRequirement already satisfied: loguru in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (0.7.3)\nRequirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (3.1.5)\nRequirement already satisfied: scikit-learn in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (1.3.2)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (1.4.2)\nRequirement already satisfied: shap in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (0.47.2)\nRequirement already satisfied: imbalanced-learn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages (0.13.0)\nCollecting xgboost\n  Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: text-unidecode>=1.3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from python-slugify) (1.3)\nRequirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.5.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from shap) (1.5.3)\nRequirement already satisfied: tqdm>=4.27.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /databricks/python3/lib/python3.12/site-packages (from shap) (24.1)\nRequirement already satisfied: slicer==0.0.8 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from shap) (0.0.8)\nRequirement already satisfied: numba>=0.54 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from shap) (0.61.2)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.12/site-packages (from shap) (2.2.1)\nRequirement already satisfied: typing-extensions in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from shap) (4.13.2)\nRequirement already satisfied: sklearn-compat<1,>=0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages (from imbalanced-learn) (0.1.3)\nCollecting nvidia-nccl-cu12 (from xgboost)\n  Downloading nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from numba>=0.54->shap) (0.44.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->shap) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->shap) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\nDownloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/253.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.2/253.9 MB\u001B[0m \u001B[31m341.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m128.5/253.9 MB\u001B[0m \u001B[31m343.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m202.4/253.9 MB\u001B[0m \u001B[31m350.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m253.8/253.9 MB\u001B[0m \u001B[31m351.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m253.9/253.9 MB\u001B[0m \u001B[31m92.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (318.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/318.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m48.5/318.1 MB\u001B[0m \u001B[31m242.6 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m96.2/318.1 MB\u001B[0m \u001B[31m238.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m142.6/318.1 MB\u001B[0m \u001B[31m235.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m192.9/318.1 MB\u001B[0m \u001B[31m239.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m244.1/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m294.6/318.1 MB\u001B[0m \u001B[31m243.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m318.0/318.1 MB\u001B[0m \u001B[31m241.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m318.1/318.1 MB\u001B[0m \u001B[31m77.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n\u001B[?25l\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/2\u001B[0m [nvidia-nccl-cu12]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/2\u001B[0m [xgboost]\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/2\u001B[0m [xgboost]\n\u001B[?25h\n\u001B[1A\u001B[2KSuccessfully installed nvidia-nccl-cu12-2.26.5 xgboost-3.0.2\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-slugify\n",
    "#!pip install loguru\n",
    "#!pip install openpyxl\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install python-slugify loguru openpyxl scikit-learn joblib shap imbalanced-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f39f94-3b5e-4dfe-b9cb-ed16e0bcb591",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bibliotecas"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from uuid import uuid4\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql.functions import split, trim, col, substring\n",
    "from pyspark.sql.types import StringType, DateType, FloatType, IntegerType\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from loguru import logger\n",
    "from uuid import uuid4\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "from slugify import slugify\n",
    "from functools import partial\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql.functions import split, trim, col, substring\n",
    "from pyspark.sql.types import StringType, DateType, FloatType, IntegerType\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab899ae1-f12e-4d90-bbeb-3d26179933d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sharepoint"
    }
   },
   "outputs": [],
   "source": [
    "## Permite a conexão com Sharepoint\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "delta = SparkSession.builder.appName(\"\").getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "secret_dict = {\n",
    "'Databricks':'key-trdg-bricks-token',\n",
    "'Sharepoint':'key-trdg-client-secret',\n",
    "'FTP':'key-trdg-ftp',\n",
    "'SAS':'key-trdg-sas',\n",
    "}\n",
    "\n",
    "def get_secret(which: str):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"keyvault\", key=secret_dict[which])\n",
    "    except:\n",
    "        print('The secrets available are: Databricks[Token], Sharepoint[Token], FTP[Password] and SAS[User and Password].')\n",
    "\n",
    "\n",
    "URL_TOKEN = 'https://login.microsoftonline.com/24090322-b104-494d-a1d3-662da14cddd4/oauth2/v2.0/token'\n",
    "URL_MSG = 'https://graph.microsoft.com/v1.0/sites/'\n",
    "\n",
    "BLOB_PATH = 'abfss://sandbox@adltrdgwestus.dfs.core.windows.net/'\n",
    "\n",
    "#spark.conf.set(\n",
    "#    \"fs.azure.account.key.adltrdgwestus.dfs.core.windows.net\",\n",
    "#    dbutils.secrets.get(scope=\"scope-keyvault-prd\", key=\"secret-databricks-trdg-prd\"))\n",
    "\n",
    "\n",
    "class Sharepoint(ABC):\n",
    "    \"\"\"Class that implements the logic to extract and load\n",
    "    datasets from the Sharepoint to the Databricks.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_file: str, name_site: str = 'DadosBI', host: str = 'achelaboratorios.sharepoint.com'):\n",
    "        self.path_file = path_file \n",
    "        self.name_site = name_site \n",
    "        self.host = host\n",
    "\n",
    "        self.headers = self.get_bearer_token()\n",
    "        self.data = self.download_file()\n",
    "\n",
    "    \n",
    "    def get_bearer_token(self) -> dict:\n",
    "        body = {'grant_type': 'client_credentials', \n",
    "                'client_id': '01c78346-6928-48c4-8cd6-d0ef71ec7021', \n",
    "                'client_secret': get_secret(which='Sharepoint'),\n",
    "                'scope': 'https://graph.microsoft.com/.default'\n",
    "                }\n",
    "        \n",
    "        headers = CaseInsensitiveDict()\n",
    "        headers[\"Accept\"] = \"application/json\"\n",
    "        headers[\"Authorization\"] = f\"Bearer {json.loads(requests.post(URL_TOKEN, data=body).text)['access_token']}\"\n",
    "\n",
    "        return headers\n",
    "\n",
    "\n",
    "    def get_ids(self, is_file=True) -> str:\n",
    "        id_site = (requests\n",
    "                   .get(f'{URL_MSG}/{self.host}:/sites/'+self.name_site+'?$select=id', \n",
    "                        headers=self.headers)\n",
    "                   .json()['id']\n",
    "                   .split(',')\n",
    "                   [1]\n",
    "                   )\n",
    "        id_drives = (requests\n",
    "                    .get(URL_MSG+id_site+f\"/drive\", \n",
    "                                 headers=self.headers)\n",
    "                    .json()\n",
    "                    ['id']\n",
    "                    )\n",
    "        if is_file:\n",
    "            url_content = URL_MSG+id_site+'/drives/'+id_drives+f'/root:/{self.path_file}:/content'\n",
    "        else: \n",
    "            url_content = URL_MSG+id_site+'/drives/'+id_drives+f'/root:/{self.path_file}:/children'\n",
    "        return url_content\n",
    "    \n",
    "\n",
    "    def download_file(self) -> dict:\n",
    "        if '.' in self.path_file:\n",
    "            url_content = self.get_ids()\n",
    "        else:\n",
    "            url_content = self.get_ids(is_file=False)\n",
    "        return (requests.get(url_content, headers=self.headers)).content\n",
    "    \n",
    "    def ls(self) -> list:\n",
    "        data_dict = json.loads(self.data)\n",
    "        list_files = []\n",
    "        for files in data_dict['value']:\n",
    "            list_files.append(files['name'])\n",
    "        return list_files\n",
    "    \n",
    "    def read_file(self, enconding:str = 'utf-8', **params) -> pd.DataFrame:\n",
    "        type_file = self.path_file.split('.')[1]\n",
    "        if type_file in ['txt', 'csv']:\n",
    "            self.read_data = pd.read_csv(io.StringIO(self.data.decode(enconding)), **params)\n",
    "            return self\n",
    "        elif type_file in ['xls', 'xlsx']:\n",
    "            self.read_data = pd.read_excel(self.data, **params)\n",
    "            return self\n",
    "        else:\n",
    "            print('Este tipo de arquivo não está implementado!')\n",
    "    \n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        return self.read_data\n",
    "    \n",
    "\n",
    "    def export_blob_storage_csv(self, path:str, delimiter:str = ';', encoding:str = \"UTF-8\") -> str:\n",
    "        ps.from_pandas(self.read_data).to_spark() \\\n",
    "        .coalesce(1) \\\n",
    "            .write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"delimiter\", delimiter) \\\n",
    "            .option(\"encoding\",encoding) \\\n",
    "            .csv(BLOB_PATH + path)\n",
    "\n",
    "\n",
    "    def slugify_columns(self, columns):\n",
    "        slug = partial(slugify, separator=\"_\")\n",
    "        return [slug(column.replace(\"%\", \"percent\").replace(\"+\", \"_\")) for column in columns]\n",
    "\n",
    "\n",
    "    def export_blob_storage_parquet(self, path:str, compression:str = 'snappy') -> str:\n",
    "        df = self.read_data\n",
    "        df.columns = self.slugify_columns(df.columns)\n",
    "        ps.from_pandas(df).to_spark() \\\n",
    "        .coalesce(1) \\\n",
    "            .write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"compression\", compression) \\\n",
    "            .parquet(BLOB_PATH + path)\n",
    "    \n",
    "    \n",
    "    def export_datalake(self, path) -> str:\n",
    "        ps.from_pandas(self.read_data).to_spark() \\\n",
    "         .write \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .option(\"overwriteSchema\", \"true\") \\\n",
    "         .saveAsTable(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411cb6f5-59f4-4134-abe6-b90caf5e1133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df, fit_scaler=True, scaler=None):\n",
    "    logger.info(\"Iniciando pré-processamento...\")\n",
    "    \n",
    "    df = df.dropna()\n",
    "    features = [\n",
    "        'SEGMENTO', 'PRIMEIRA_COMPRA', \n",
    "        'PRIMEIRA_COMPRA_CRM_LISTA_DE_BLOQUEIO_perc',\n",
    "        'PRIMEIRA_COMPRA_CPF_INATIVO_perc', \n",
    "        'QTD_MARCA_AVG', 'QTD_MARCA_qt50'\n",
    "    ]\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    Y = df['Desvio'].map({'N': 0, 'S': 1}).astype(int)\n",
    "    \n",
    "    X['SEGMENTO'] = LabelEncoder().fit_transform(X['SEGMENTO'])\n",
    "    \n",
    "    if fit_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "    logger.success(\"Pré-processamento finalizado.\")\n",
    "    return X_scaled, Y.values, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981fae4c-0913-42bf-a7b9-096c87cd722d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(X, Y):\n",
    "    logger.info(\"Treinando modelo...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, Y_resampled = smote.fit_resample(X, Y)\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_resampled, Y_resampled)\n",
    "    logger.success(\"Modelo treinado.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52cdacd-e7bd-4806-b514-faed6e4e63d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_model(model, X, Y):\n",
    "    logger.info(\"Validando modelo com cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(model, X, Y, cv=skf, scoring='accuracy')\n",
    "    logger.success(f\"Acurácia média: {scores.mean():.4f}\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ec9d8a-35e0-4d47-8cb7-e07373a31c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, X_test, Y_test):\n",
    "    logger.info(\"Avaliando modelo...\")\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    print(f\"Accuracy: {accuracy_score(Y_test, Y_pred):.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(Y_test, Y_pred):.4f}\")\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542cb917-0dfa-45fd-bf02-f16b0932124d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, scaler, model_path='modelo_xgb.pkl', scaler_path='scaler.pkl'):\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    logger.success(\"Modelo e scaler salvos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5ba6ec-df48-465b-ab9c-3164742268d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_new_data(df_new, model, scaler):\n",
    "    logger.info(\"Realizando predição em novos dados...\")\n",
    "    X_new, _, _ = preprocess_data(df_new, fit_scaler=False, scaler=scaler)\n",
    "    preds = model.predict(X_new)\n",
    "    df_new['Desvio_predito'] = preds\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23368f7-9dda-4897-bf9e-93d4ffeba9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-05-29 14:42:11.926\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mpreprocess_data\u001B[0m:\u001B[36m2\u001B[0m - \u001B[1mIniciando pré-processamento...\u001B[0m\n\u001B[32m2025-05-29 14:42:11.933\u001B[0m | \u001B[32m\u001B[1mSUCCESS \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mpreprocess_data\u001B[0m:\u001B[36m23\u001B[0m - \u001B[32m\u001B[1mPré-processamento finalizado.\u001B[0m\n\u001B[32m2025-05-29 14:42:11.936\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mtrain_model\u001B[0m:\u001B[36m2\u001B[0m - \u001B[1mTreinando modelo...\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d2876d1418419c92da4d77322e9974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:42:25] WARNING: /workspace/src/learner.cc:738: \nParameters: { \"use_label_encoder\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n\u001B[32m2025-05-29 14:42:26.298\u001B[0m | \u001B[32m\u001B[1mSUCCESS \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mtrain_model\u001B[0m:\u001B[36m16\u001B[0m - \u001B[32m\u001B[1mModelo treinado.\u001B[0m\n\u001B[32m2025-05-29 14:42:26.299\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mvalidate_model\u001B[0m:\u001B[36m2\u001B[0m - \u001B[1mValidando modelo com cross-validation...\u001B[0m\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:42:26] WARNING: /workspace/src/learner.cc:738: \nParameters: { \"use_label_encoder\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:42:26] WARNING: /workspace/src/learner.cc:738: \nParameters: { \"use_label_encoder\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:42:26] WARNING: /workspace/src/learner.cc:738: \nParameters: { \"use_label_encoder\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:42:26] WARNING: /workspace/src/learner.cc:738: \nParameters: { \"use_label_encoder\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-4f7ecd26-5ece-45e8-82fa-cd01e3081d52/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [14:42:26] WARNING: /workspace/src/learner.cc:738: \nParameters: { \"use_label_encoder\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n\u001B[32m2025-05-29 14:42:27.662\u001B[0m | \u001B[32m\u001B[1mSUCCESS \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mvalidate_model\u001B[0m:\u001B[36m5\u001B[0m - \u001B[32m\u001B[1mAcurácia média: 0.6900\u001B[0m\n\u001B[32m2025-05-29 14:42:27.663\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mevaluate_model\u001B[0m:\u001B[36m2\u001B[0m - \u001B[1mAvaliando modelo...\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.85      0.94      0.89        31\n           1       0.67      0.44      0.53         9\n\n    accuracy                           0.82        40\n   macro avg       0.76      0.69      0.71        40\nweighted avg       0.81      0.82      0.81        40\n\nAccuracy: 0.8250\nROC AUC: 0.6900\nMatriz de Confusão:\n [[29  2]\n [ 5  4]]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-05-29 14:42:48.382\u001B[0m | \u001B[32m\u001B[1mSUCCESS \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36msave_model\u001B[0m:\u001B[36m4\u001B[0m - \u001B[32m\u001B[1mModelo e scaler salvos.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Ler os dados\n",
    "df = Sharepoint('General/Arquivo_Databricks/Aprendizagem_fraude_teste.xlsx').read_file(engine = 'openpyxl').to_dataframe()\n",
    "\n",
    "# 2. Pré-processar\n",
    "X, Y, scaler = preprocess_data(df)\n",
    "\n",
    "# 3. Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "# 4. Treinar\n",
    "modelo_xgb = train_model(X_train, y_train)\n",
    "\n",
    "# 5. Validar\n",
    "validate_model(modelo_xgb, X, Y)\n",
    "\n",
    "# 6. Avaliar\n",
    "evaluate_model(modelo_xgb, X_test, y_test)\n",
    "\n",
    "# 7. Salvar\n",
    "save_model(modelo_xgb, scaler)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML - Análise Fraudes XGBoost",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}